name: TRTLLM_inference

networks:
  server:
    driver: bridge

services:
  tensorrt_llm:
    image: nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc3
    container_name: TensorRTLLM_Inference
    env_file:
      - ../.env
    networks:
      - server
    ports:
      - "8000:8000"
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
      nofile:
        soft: 1048576
        hard: 1048576
    gpus: all
    environment:
      - HF_HOME=/data/hf
      - HF_HUB_CACHE=/data/hf/hub
    volumes:
      - ${HOME}/.cache/huggingface:/data/hf
      - /dev/null:/etc/ld.so.conf.d/00-cuda-compat.conf
    command:
      - trtllm-serve
      - serve
      - Qwen/Qwen3-32B-FP8
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --tp_size
      - "1"
      - --kv_cache_free_gpu_memory_fraction
      - "0.80"
      - --max_seq_len
      - "10000"
      - --trust_remote_code
